{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960231ec-94db-4f1a-8125-56a87ddd9435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "os.chdir(\"..\")\n",
    "# !{sys.executable} -m pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f6082e-46d5-403b-89a8-38f5dabe2347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to datasets/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from basix import files\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from llm import data, models\n",
    "from llm.config import config\n",
    "from llm.tokenize import SentencesTokenizer\n",
    "from llm.embed import CBOWEmbedder\n",
    "\n",
    "nltk.download('punkt', download_dir = \"datasets/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068aac85-74a0-4ce5-b0e8-f4f922d3ba39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_len = 100\n",
    "title_len = 20\n",
    "latent_dim = 256\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e819a66a-9f7c-4392-9765-6db1a2d5882b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 22:04:03.066 | DEBUG    | llm.data:load_corpus:18 - Corpus already exists in datasets/raw/brazilian-news.parquet. Skipping downloading corpus.\n",
      "2023-03-22 22:04:03.066 | DEBUG    | llm.data:load_corpus:20 - Importing news from datasets/raw/brazilian-news.parquet\n",
      "2023-03-22 22:04:04.781 | DEBUG    | llm.data:load_corpus:38 - Using a sample of size 10000\n",
      "2023-03-22 22:04:04.802 | DEBUG    | llm.data:load_corpus:43 - Importing news titles from datasets/raw/title.parquet\n",
      "2023-03-22 22:04:04.803 | DEBUG    | llm.data:load_corpus:46 - Importing news texts from datasets/raw/text.parquet\n",
      "2023-03-22 22:04:05.299 | DEBUG    | llm.models:transform:37 - Tokenizing sentences\n",
      "2023-03-22 22:04:17.819 | DEBUG    | llm.models:transform:46 - Getting embedding vectors\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:03<00:00, 2544.32it/s]\n",
      "2023-03-22 22:04:21.883 | DEBUG    | llm.models:transform:37 - Tokenizing sentences\n",
      "2023-03-22 22:04:22.132 | DEBUG    | llm.models:transform:46 - Getting embedding vectors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 104570.29it/s]\n",
      "2023-03-22 22:04:22.247 | DEBUG    | llm.models:transform:37 - Tokenizing sentences\n",
      "2023-03-22 22:04:22.500 | DEBUG    | llm.models:transform:46 - Getting embedding vectors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 107759.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 s, sys: 1.07 s, total: 19.6 s\n",
      "Wall time: 19.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "texts, titles = data.load_corpus(sample=10000)\n",
    "\n",
    "processer = models.TextProcesser(verbose=1)\n",
    "\n",
    "texts_vecs = processer.transform(texts, maxlen=text_len)\n",
    "\n",
    "titles_input_vecs = processer.transform(titles, maxlen=title_len, add_bos=True)\n",
    "titles_output_vecs = processer.transform(titles, maxlen=title_len, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bcf72de-6b20-4c31-b773-34594e1551d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_inp_train, y_inp_test, y_out_train, y_out_test = \\\n",
    "    train_test_split(texts_vecs, titles_input_vecs, titles_output_vecs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48308c6b-4213-43b3-8d11-e9f42fd684a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None, config.CBOW_VECTOR_SIZE))\n",
    "\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, config.CBOW_VECTOR_SIZE))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(config.CBOW_VECTOR_SIZE)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d960e26-a706-412f-a3a5-28064a30369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  keras.backend.sum(keras.backend.square( y_true - y_pred )) \n",
    "    SS_tot = keras.backend.sum(keras.backend.square( y_true - keras.backend.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + keras.backend.epsilon()) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd5cc5f6-dd33-4ae9-8b78-0898a162e4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f932e-fdaa-442e-9871-9a77b19e7a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "128/128 [==============================] - 19s 140ms/step - loss: 3.4263 - mse: 3.4263 - mae: 1.1327 - r2_keras: 0.1300 - val_loss: 3.2238 - val_mse: 3.2238 - val_mae: 1.0919 - val_r2_keras: 0.1830\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 18s 137ms/step - loss: 3.1251 - mse: 3.1251 - mae: 1.0725 - r2_keras: 0.2063 - val_loss: 3.1124 - val_mse: 3.1124 - val_mae: 1.0725 - val_r2_keras: 0.2113\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 18s 138ms/step - loss: 3.0188 - mse: 3.0188 - mae: 1.0499 - r2_keras: 0.2333 - val_loss: 3.0707 - val_mse: 3.0707 - val_mae: 1.0573 - val_r2_keras: 0.2218\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 18s 137ms/step - loss: 2.9498 - mse: 2.9498 - mae: 1.0378 - r2_keras: 0.2508 - val_loss: 3.0353 - val_mse: 3.0353 - val_mae: 1.0509 - val_r2_keras: 0.2307\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 17s 136ms/step - loss: 2.8843 - mse: 2.8843 - mae: 1.0212 - r2_keras: 0.2674 - val_loss: 3.0315 - val_mse: 3.0315 - val_mae: 1.0547 - val_r2_keras: 0.2317\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 18s 137ms/step - loss: 2.8273 - mse: 2.8273 - mae: 1.0093 - r2_keras: 0.2819 - val_loss: 3.0227 - val_mse: 3.0227 - val_mae: 1.0422 - val_r2_keras: 0.2339\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 18s 144ms/step - loss: 2.7801 - mse: 2.7801 - mae: 0.9990 - r2_keras: 0.2941 - val_loss: 3.0224 - val_mse: 3.0224 - val_mae: 1.0398 - val_r2_keras: 0.2339\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 18s 138ms/step - loss: 2.7302 - mse: 2.7302 - mae: 0.9883 - r2_keras: 0.3066 - val_loss: 3.0336 - val_mse: 3.0336 - val_mae: 1.0417 - val_r2_keras: 0.2310\n",
      "Epoch 9/100\n",
      " 31/128 [======>.......................] - ETA: 11s - loss: 2.7040 - mse: 2.7040 - mae: 0.9858 - r2_keras: 0.3198"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(loss='mse', optimizer='adam',  metrics=['mse', 'mae', r2_keras])\n",
    "\n",
    "model.fit(\n",
    "    [X_train, y_inp_train],\n",
    "    y_out_train,\n",
    "    batch_size=50,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Save model\n",
    "ENCODER_DECODER_PATH = os.path.join(config.MODEL_PATH, f\"version={config.MODEL_VERSION}\", \"encoder-decoder.bin\")\n",
    "model.save(ENCODER_DECODER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634e655-5657-41e3-8ed4-58df91fcc1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec00c7-6053-4a11-a074-107e1b18f2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0dfca-9e63-41e9-887b-52c8114e1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model = keras.models.load_model(\"s2s\")\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc04c8ab-1ea7-4b09-9671-2aabf5dd7e17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.from_tensor_slices_op.TensorSliceDataset"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "slices = tf.data.Dataset.from_tensor_slices(texts_vecs)\n",
    "type(slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a9470-60ef-4738-8589-b190ae51b47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42ec53-0f26-4082-8061-df00801a62fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c27c48-de69-467c-8e45-c43be912a57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80674b7a-9222-45b1-a7d0-8c454eeef8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.8422527 ,  1.5201029 ,  3.6546187 , ..., -2.1878119 ,\n",
       "         1.2598858 ,  3.8599381 ],\n",
       "       [ 0.15048188, -2.5627968 , -5.4844007 , ...,  3.2361865 ,\n",
       "         4.6852965 ,  2.4972484 ],\n",
       "       [ 0.3609909 , -0.23715064,  3.7527487 , ...,  3.8275928 ,\n",
       "        -4.6390014 , -0.80124545],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f1144d-8e14-4a3b-99c8-62ec4b325a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 28, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79370a-4792-40f5-971e-debdc3f3e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences(texts_vecs, padding='post', dtype='float32', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "571fcffc-536d-4ca5-999f-efeefa9eb3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def padding(vec_lists):\n",
    "    return  pad_sequences(vec_lists, padding='post', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60dad8c8-52d3-49ec-84c9-934b403dcab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "padded = padding(texts_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d0afbc-5e02-48ef-8797-0d2af4778e27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5d185a0-d189-4496-80dc-52ec4c518216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65f2788e-33e7-4d0a-b5c9-7391cdc553da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1685.1599999999999"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series([len(x) for x in texts_vecs]).quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc250e1a-00f3-4472-b9e5-9fd31a403d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest input sequence: 4426\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(texts_vecs, key=len))\n",
    "print(f\"Length of longest input sequence: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20632215-d580-4a1e-9b62-927d036278ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest input sequence: 28\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(titles_vecs, key=len))\n",
    "print(f\"Length of longest input sequence: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81c0c2a7-ab4c-4151-a7e6-e84bce2362fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.06,  2.86, -0.39],\n",
       "        [-1.34, -1.12, -1.03],\n",
       "        [ 0.48,  0.83, -0.93],\n",
       "        [-0.54,  0.37, -0.92]]),\n",
       " array([[-0.57, -1.26,  0.6 ],\n",
       "        [-1.82,  0.39,  1.25],\n",
       "        [-0.51,  2.12, -1.35]]),\n",
       " array([[ 0.89,  1.25,  0.25],\n",
       "        [ 1.05,  1.15,  1.  ],\n",
       "        [-0.37,  0.86, -0.79],\n",
       "        [-1.69,  0.01,  0.34]]),\n",
       " array([[ 0.17,  1.33, -0.33],\n",
       "        [ 0.58, -0.54,  1.01]])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aqu ieu tenho uma lista de 4 sentenças represendas com word embedding\n",
    "# num espaço de embedding de dimenão 3. O número de palavras nas senenças\n",
    "# são respectivamente 4, 3, 4 e 2. Como utilizar o tensorflow para fazer um padding\n",
    "# de modo que todas as senteças tenha o mesmo número de palavras\n",
    "\n",
    "[np.random.randn(np.random.randint(2,6),3).round(2) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50725f-0cc0-4ccc-a20f-4136c00f7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    np.array([[0,1,0,0],[0,3,2,1],[4,6,3,6],[1,2,3,4],[6,7,9,8]]),\n",
    "    np.array([[0,1,0,0],[0,3,2,1]]),\n",
    "    np.array([[0,1,0,0],[0,3,2,1],[4,6,3,6],[1,2,3,4],[6,7,9,8]]),\n",
    "    np.array([[0,1,0,0],[0,3,2,1],[4,6,3,6],[1,2,3,4],[6,7,9,8]]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c49a2da1-d0df-44a3-989e-eb345af901eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequências originais:\n",
      "[[ 0.06  2.86 -0.39]\n",
      " [-1.34 -1.12 -1.03]\n",
      " [ 0.48  0.83 -0.93]\n",
      " [-0.54  0.37 -0.92]]\n",
      "[[-0.57 -1.26  0.6 ]\n",
      " [-1.82  0.39  1.25]\n",
      " [-0.51  2.12 -1.35]]\n",
      "[[ 0.89  1.25  0.25]\n",
      " [ 1.05  1.15  1.  ]\n",
      " [-0.37  0.86 -0.79]\n",
      " [-1.69  0.01  0.34]]\n",
      "[[ 0.17  1.33 -0.33]\n",
      " [ 0.58 -0.54  1.01]]\n",
      "\n",
      "Sequências com padding:\n",
      "[[[ 0.06  2.86 -0.39]\n",
      "  [-1.34 -1.12 -1.03]\n",
      "  [ 0.48  0.83 -0.93]\n",
      "  [-0.54  0.37 -0.92]]\n",
      "\n",
      " [[-0.57 -1.26  0.6 ]\n",
      "  [-1.82  0.39  1.25]\n",
      "  [-0.51  2.12 -1.35]\n",
      "  [ 0.    0.    0.  ]]\n",
      "\n",
      " [[ 0.89  1.25  0.25]\n",
      "  [ 1.05  1.15  1.  ]\n",
      "  [-0.37  0.86 -0.79]\n",
      "  [-1.69  0.01  0.34]]\n",
      "\n",
      " [[ 0.17  1.33 -0.33]\n",
      "  [ 0.58 -0.54  1.01]\n",
      "  [ 0.    0.    0.  ]\n",
      "  [ 0.    0.    0.  ]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Definir as sequências de entrada\n",
    "sentences = [\n",
    "    np.array([[ 0.06,  2.86, -0.39],\n",
    "              [-1.34, -1.12, -1.03],\n",
    "              [ 0.48,  0.83, -0.93],\n",
    "              [-0.54,  0.37, -0.92]]),\n",
    "    np.array([[-0.57, -1.26,  0.6 ],\n",
    "              [-1.82,  0.39,  1.25],\n",
    "              [-0.51,  2.12, -1.35]]),\n",
    "    np.array([[ 0.89,  1.25,  0.25],\n",
    "              [ 1.05,  1.15,  1.  ],\n",
    "              [-0.37,  0.86, -0.79],\n",
    "              [-1.69,  0.01,  0.34]]),\n",
    "    np.array([[ 0.17,  1.33, -0.33],\n",
    "              [ 0.58, -0.54,  1.01]])\n",
    "]\n",
    "\n",
    "# Fazer o padding das sequências\n",
    "padded_sequences = pad_sequences(sentences, padding='post', dtype='float32')\n",
    "\n",
    "# Imprimir as sequências de entrada originais\n",
    "print(\"Sequências originais:\")\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "\n",
    "# Imprimir as sequências com padding\n",
    "print(\"\\nSequências com padding:\")\n",
    "print(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d5a576-cc11-434f-b986-ae1925fb21cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n"
     ]
    }
   ],
   "source": [
    "print(len(texts_vecs), len(titles_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0d84a16-dea5-41ea-9599-c88dce96a35f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁municípios',\n",
       "  '▁com',\n",
       "  '▁regimes',\n",
       "  '▁próprios',\n",
       "  '▁de',\n",
       "  '▁previdência',\n",
       "  '▁terão',\n",
       "  '▁dívidas',\n",
       "  '▁renego',\n",
       "  'ciadas']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processer.get_tokens_from_vectors(titles_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "258093fb-5263-4357-b260-12c0490f3139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁teste'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processer.get_most_similar_token(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88edc755-04ac-4752-ad1e-ca3c375870e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method KeyedVectors.similar_by_vector of <gensim.models.keyedvectors.KeyedVectors object at 0x7f32d0b3fac0>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.wv.similar_by_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "823d7e98-1fe4-4848-8496-148206431eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 50)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = processer.tokenize([\"teste de mensagem. Hoje eu vou.\", \"Agora desisti\"])\n",
    "transf = processer.get_vectors(tokens)\n",
    "transf[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37eb7c0e-fd4d-49ee-a607-9dda0ca5b496",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "820e21b5-c8a5-4284-977f-6f3b65cab18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Parabéns por mostrar que a filatelia (que é o somatório de arte, educação, cultura, lazer e terapia) continua sendo o colecionismo que ainda agrada e atrai jovens de 8 a 80 anos (\"Figurinhas raras, \"Folhinha\", 16/5).  JOSÉ ANTONIO BITTENCOURT FERRAZ, presidente do Clube Filatélico e Numismático de Lorena (Lorena, SP)  *  Gostei muito da reportagem. Tudo o que envolve coleções tem minha atenção, pois envolve organização, atenção e carinho. Tenho uma coleção especial: fios de cabelos. São quase mil fios, uma coleção com mais de 30 anos (tenho 46) e fios de pessoas que passaram por minha vida.  SIMONE GIUDICI (São Paulo, SP)    *  PARTICIPAÇÃO  Os leitores podem colaborar com o conteúdo da Folha enviando notícias, fotos e vídeos (de acontecimentos ou comentários) que sejam relevantes no Brasil e no mundo. Para isso, basta acessar Envie sua Notícia ou enviar mensagem para leitor@uol.com.br',\n",
       " 'A reedição de \"Viagem Pitoresca e Histórica ao Brasil\", de Jean-Baptiste Debret, reflete a longa tradição da inquietação do Brasil, desde a formação da sua identidade nacional, para lidar com as representações da sua realidade no exterior.  Convidado ao país para ajudar a retratar o Brasil que se construía com a vinda da corte portuguesa, ele produziu um vasto documentário sobre a vida da nova nação.  Sua narrativa, em textos e gravuras, se consolidou como um clássico da formação da imagem internacional do Brasil. Ela revela como o país, que se consolidava, era interpretado por estrangeiros.    Esse exercício de alteridade construiu uma imagem de exotismo. Debret mostrava essa nova sociedade, formada por europeus, povos indígenas e escravos africanos, em paisagem humana e natural, estranha a um francês.  A imagem de nação exótica pegou, e se tornou por mais de um século a principal referência sobre o que o resto do mundo pensava sobre o Brasil – e talvez esteja ainda presente na \"marca\" do país no mundo atual.  Desde o século 19, o país lida de forma ambígua com essa imagem. Por um lado, luta para desfazê-la e se mostrar moderno e civilizado. Por outro, usa parte dessas características pictóricas (como as riquezas naturais) para se promover no exterior.  A ascensão da marca global do Brasil nos últimos anos, especialmente o crescimento econômico registrado no fim da década passada, parecem ter afastado o exotismo. O país hoje é mais conhecido, e menos estranho ao resto do mundo.  Ainda assim, a inquietação com a imagem e a ambiguidade na promoção do país continuam existindo.  Estudos de \"nation branding\", que avaliam a reputação do Brasil, indicam problemas na sua imagem. O país é visto como decorativo, um ótimo destino para férias, festas e aventuras, mas pouco sério para negócios.  Essa imagem se fortaleceu durante a Copa do Mundo, vista como grande festa global, em meio a um país cheio de problemas. A probabilidade é de que isso se repita nos relatos pictóricos dos estrangeiros que virão à Olimpíada.',\n",
       " 'O Ministério da Justiça determinou que a Polícia Federal abra inquérito para investigar as ameaças de morte ao ministro Edinho Silva (Comunicação Social) registradas na Internet.  Um internauta crítico ao PT publicou em uma página do ministro nas redes sociais um comentário afirmando que Edinho e os petistas vão morrer.  \"Olha Edição Silva [sic], filho da puta, quem vai morrer é você e os petistas\", escreveu o internauta. \"Não ande tão tranquilo. Já tem gente na sua cola. Comunista filho da puta.\".  As ameaças foram publicadas abaixo do link de uma entrevista em que o ministro pedia a retomada do diálogo. De acordo com o Ministério da Justiça, a PF já deu início à apuração do episódio.  O autor das postagens tem um perfil ativo nas redes sociais. Ele publica textos a favor do impeachment da presidente Dilma Rousseff, no Facebook, além de uma série de notícias relacionadas ao escândalo da Petrobras.  O autor das postagens tem um perfil ativo nas redes sociais. Ele publica textos contra o PT e a favor do impeachment da presidente Dilma Rousseff, no Facebook, além de uma série de notícias relacionadas ao escândalo da Petrobras.  A última mensagem postada pelo autor da ameaça é um vídeo que prega a intervenção militar e que cita, de forma distorcida, a fala feita por Edinho nesta quinta.  \"Edinho Silva veio falar que se nós não baixarmos o tom vai morrer gente. Estamos sendo ameaçados por esses vermelhos filhos da mãe. Cadê o povo, cadê o Exército, quem vai nos defender?\", indaga o homem que aparece no filme.  Edinho classificou a ameaça como \"mais um exemplo do grau de intolerância que existe no país\".  \"Hoje as pessoas falam em matar de uma forma simplista, como se isso não significasse nada. Mais uma vez digo que é preciso que todas as lideranças que têm responsabilidade com o país tomem iniciativas que combatam a intolerância\", afirmou.',\n",
       " 'O prefeito Fernando Haddad (PT) anunciou nesta terça-feira (26) um pacote anticorrupção que prevê desde demissão de servidores públicos que não comprovem a origem de seus bens à proibição de aceitar brindes com valor maior do que R$ 100.  A primeira regra estará num projeto de lei que altera o Estatuto do Servidor, prevendo a punição ao funcionário investigado por enriquecimento ilícito.  Desde o ano passado, os cerca de 155 mil funcionários públicos da cidade são obrigados a declarar seus bens à prefeitura.  A segunda está prevista num código de ética municipal que terá de ser seguido pelos agentes da administração. Segundo Haddad, o código que define quais são os atos de imoralidade do servidor será instituído por meio de um decreto.  A intenção é barrar casos de enriquecimento ilícito –que não é considerado crime no país– e agilizar a demissão de servidores corruptos na administração.  Com a alteração do estatuto, também fica mais clara a proibição de nepotismo (contratação de parentes), regra que não é bem descrita no estatuto, segundo a prefeitura.  Já o novo código de ética também proíbe prestação de serviços a pessoas físicas ou jurídicas que tenham interesse em decisões do município, além do vazamento de informações sigilosas.  Também será vetado o uso de viagens de trabalho para comparecer a eventos político-eleitorais.  O anúncio foi feito durante a divulgação de um balanço de dois anos da criação da CGM (Controladoria Geral do Município).  O órgão foi responsável por investigações como a da Máfia do ISS, que, em parceria com o Ministério Público, identificou 13 auditores fiscais suspeitos de envolvimento numa rede de corrupção no momento da arrecadação do imposto. Os 13 foram demitidos e outros 70 são investigados.  Os auditores envolvidos no esquema ofereciam a empresas descontos na cobrança do tributo em troca de propina. A apuração mostrou que um terço do que era devido não era arrecadado na cidade.  ARRECADAÇÃO  Haddad anunciou que, em dois anos, o trabalho da Controladoria vai garantir a devolução de R$ 270 milhões aos cofres públicos na arrecadação de impostos.  Desse total, cerca de R$ 100 milhões já retornaram ao caixa do município, os demais devem ser recuperados.  Só em relação à máfia do ISS, a previsão é reaver R$ 190 milhões -R$ 45 milhões já em caixa. O restante do valor refere-se a auditorias internas em contratos nas áreas de Saúde, Serviços, Esportes, Habitacão, entre outros.  Pelo menos 505 empreendimentos foram autuados por terem recolhido valores menores de tributo.',\n",
       " 'Entrevistei homens e mulheres que enfrentaram inúmeras dificuldades ao longo de suas vidas. Muitos afirmaram que faz parte do processo de amadurecimento saber lidar com os problemas de tal forma que, além de buscar a solução possível, consigam minimizar o sofrimento inevitável.  Uma atriz de 59 anos disse: \"Encontrei meu ex-marido passeando com a namorada. Parecia que ele estava se exibindo, querendo provar que estava muito mais feliz sem mim, que a vida dele melhorou depois da nossa separação. Se eu fosse mais nova iria chorar, ficar triste, ligar para minhas amigas para dizer que ele é um grande babaca. Mas eu me surpreendi com a minha reação. Decidi não me fazer de vítima e percebi que não quero ter na minha vida um homem que faz questão de me fazer sofrer. Resolvi ligar o botão do foda-se e deletá-lo da minha vida para sempre\".  Um professor de 67 anos considera que a conquista da maturidade é resultado de aprender a lidar cada vez melhor com os problemas.  \"Para mim o que importa não é o problema, mas a maneira como eu lido com ele. Sempre que aparece uma dificuldade eu tento me acalmar e dizer para mim mesmo: \\'Como eu vou administrar este problema?\\' Em vez de entrar em pânico, eu busco uma saída. E, se ela não existir, procuro uma forma mais tranquila de lidar com o problema para que ele não afete toda a minha vida. Descobri que a resposta nem sempre é sim ou não, mas algo no meio do caminho.\"  O professor já encontrou muitas soluções com o \"caminho do meio\".  \"Uma vez fui convidado para participar de um importante evento científico. Queria responder que sim, mas isso iria me custar muito em termos de saúde. Se eu respondesse que não, perderia uma excelente oportunidade profissional. Sugeri que a minha participação fosse por Skype e eles aceitaram. Não precisei sair de casa e foi um sucesso\".  Ele afirma que o segredo é focar no ganho que pode ter cada vez que consegue enfrentar as dificuldades com equilíbrio: a conquista da maturidade necessária para administrar os novos problemas que, com certeza, virão no futuro. Ele conclui: \"Minha frase preferida é: \\'a vida te ensina a viver, se você viver o suficiente\\'\".  E você, já experimentou resolver seus problemas com o \"caminho do meio\"? Qual é a sua fantasia?',\n",
       " 'O BSI, banco suíço em processo de venda para o rival EFG International, registrou saída líquida de novos recursos no segundo trimestre de 6,3 bilhões de francos suíços (US$ 6,4 bilhões). A retirada ocorre em meio a sanções sobre laços comerciais com um fundo governamental da Malásia afetado por um escândalo.  A informação sobre a saída líquida de recursos do BSI foi divulgada pelo Grupo BTG Pactual, que atualmente detém o BSI. O BTG Pactual disse na terça-feira (9) que o lucro caiu no segundo trimestre, parcialmente após a receita com gestão de fortunas sofrer com as saídas de recursos do BSI.  O banco com sede em Lugano, na Suíça, move um recurso contra uma decisão de maio do regulador financeiro suíço FINMA, que diz que o BSI violou regras sobre lavagem de dinheiro por meio de uma relação comercial e transações ligadas ao 1Malaysia Development Bhd.  Acredita-se que o fundo da Malásia conhecido como 1MDB realizou US$ 4 bilhões em negócios irregulares nos últimos anos. Investigadores em diversos países estão tentando determinar se transações relacionadas entre o 1MDB, bancos e clientes foram direcionadas para as contas de influentes políticos.  A FINMA ordenou que o BSI entregue lucros de 95 milhões de francos suíços e que pare de operar quando for integrado à EFG. O BSI disse que a decisão da FINMA é \"incorreta\". Cingapura também ordenou que as operações do BSI na cidade-Estado sejam encerradas.',\n",
       " 'Sou professora efetiva do Estado de São Paulo da diretoria de ensino centro-sul. Designada como coordenadora do centro de línguas Ipiranga desde de junho de 2014. Por trás dessa responsabilidade há uma \"maldição\", pois, quando se recebe essa designação, seu salário fica à merce de um fenômeno: ou você sofre uma demora de 3 a 4 meses para receber como tal, ou, quando encerra o ano letivo sua designação é encerrada sem que haja qualquer comando da sua unidade de frequência. Comigo aconteceu a segunda ocorrência. A direção da escola tomou todas as providências necessárias enviando novamente documentação provando meu posto. Enviei mensagem ao CPP que pediu que aguardasse, pois houve um erro de digitação na Secretaria da Fazenda. Depois que foi divulgado que haveria folha suplementar para correção desse erro, fiquei tranquila, mas quando vi a Folha Suplementar, no site da Secretaria da Fazenda, a revolta acendeu, pois sei, de acordo com experiência de várias pessoas que, provar que você tem direito ao seu salário (!) é uma via crucis. Sou professora por opção. Trabalho, defendo a qualidade de ensino e tenho agora de arcar com prejuízos que não provoquei. Pagarão os atrasos, mas meu prejuízo já foi feito: financeiro e emocional. Parece exagero? Só quem fica frustrado diante do não cumprimento de compromissos é que sabe. Nossa situação está difícil, temos de driblar necessidades para não cair em abismos de endividamento, qualquer desajuste nos desequilibra. Seria tão bom se eu me sentisse levada a sério, necessária ao desenvolvimento, como dizem as teorias sobre educação.  *  PARTICIPAÇÃO  Os leitores podem colaborar com o conteúdo da Folha enviando notícias, fotos e vídeos (de acontecimentos ou comentários) que sejam relevantes no Brasil e no mundo. Para isso, basta acessar Envie sua Notícia ou enviar mensagem para leitor@uol.com.br',\n",
       " 'O Brasil é o maior vencedor da Liga Mundial de vôlei com nove títulos. A Itália tem oito. Sérgio Escadinha, sete.  E para manter a hegemonia na competição em busca do deca, a seleção conta com o retorno do líbero após quase três anos fora do time.  Aos 39, Escadinha volta a vestir a camisa 10 verde e amarela nesta sexta-feira (29), às 14h, na estreia da equipe comandada por Bernardinho na Liga Mundial, contra a Sérvia.  \"Eu tenho procurando não pensar muito sobre essa volta. Estou aqui para trabalhar tanto quanto os outros e mostrar as minhas qualidades. Espero conseguir\", afirma.  No Mineirinho, em Belo Horizonte, jogador e seleção com mais títulos no torneio esperam começar a reencontrar o caminho dos títulos.  \"A Liga é uma competição muito especial. Está anualmente no calendário e cada vez mais forte\", lembra.  Apesar das nove conquistas, o Brasil não levanta a taça da Liga Mundial desde 2010. Escadinha não jogou aquela edição em razão de uma cirurgia para retirada de uma hérnia de disco lombar.  Desde então, foram três vices em quatro anos na Liga. No período também vieram as pratas no Mundial-2014 e na Olimpíada-2012. Escadinha pediu dispensa da seleção após os Jogos de Londres.  \"Não foi por falta de dedicação que houve essa sequência de vices. O que as pessoas precisam entender é que queremos os títulos sempre, mais do que todo mundo, mas chegar a uma final é algo muito positivo também\", diz.  Em 2009, ano de seu sétimo título em uma sequência que começou em 2001, Escadinha foi eleito o melhor jogador da Liga Mundial. Aquela foi a primeira e ainda única vez que um líbero ganhou tal prêmio em 25 edições.  Então, na preparação para a Olimpíada do Rio, Bernardinho percebeu que precisava de novo do camisa 10. Escadinha aceitou o chamado.  \"Claro que dou um gás a mais, sou pilhado nesse sentido, e cobro muito dos meus companheiros em quadra. Mas, sou cobrado também. Eles sabem que podem exigir porque eu quero corresponder\", afirma o campeão olímpico (2004) e bi mundial (2002 e 2006).  PRÓXIMOS JOGOS  O Brasil enfrenta a Sérvia nesta sexta, às 14h, e no domingo (31), às 10h, no Mineirinho. Depois, encara a Austrália dias 5 e 7 de junho, em São Bernardo do Campo (SP).  O último adversário do Grupo A é a Itália, dias 2 e 3 de julho, em Cuiabá (MT). Antes, os brasileiros viajam para os jogos na casa dos rivais.  O Brasil já está classificado para a fase final da Liga, que acontece no Rio de Janeiro, entre 14 e 19 de julho. As disputas servirão como evento-teste da Rio-2016.  Além dos anfitriões, os jogos no Maracanãzinho terão os dois melhores dos grupos A (que ainda tem Sérvia, Itália e Austrália) e B (Irã, Polônia, Rússia, Estados Unidos), além do campeão do Grupo 2 (que no total conta com Argentina, Bulgária, Canadá, Cuba, República Tcheca, França, Japão, Coreia do Sul, Bélgica, Finlândia, Portugal e Holanda.  A seleção brasileira conquistou os títulos da Liga em 1993, 2001, 2003, 2004, 2005, 2006, 2007, 2009 e 2010, além de ter outros cinco vices.  O Brasil já sediou as finais da competição quatro vezes (1993, 1995, 2002 e 2008), mas só foi campeão na primeira destas edições, em São Paulo.  NA TV Brasil x Sérvia 14h - Globo e Band',\n",
       " 'Na defesa prévia que deverá ser protocolada no Conselho de Ética do Senado na tarde desta quinta-feira (18), o senador Delcídio do Amaral (PT-MS) vai argumentar que foram \"simples jactância [bravata]\" as menções que fez sobre suposta influência política no STF (Supremo Tribunal Federal) no sentido de libertar o ex-diretor da área internacional da Petrobras Nestor Cerveró, preso pela Operação Lava Jato.  O senador é alvo de pedido, feito por PPS e Rede, de abertura de processo de cassação do mandato por quebra de decoro parlamentar, após ter sido gravado pelo filho de Cerveró, Bernardo, falando sobre providências que estaria adotando para ajudar o ex-diretor da petroleira a escapar do foco das investigações, incluindo um plano de fuga e ajuda financeira.  Na gravação, Delcídio diz a Bernardo que já conversara com dois ministros do STF, Teori Zavascki e Dias Toffoli, sobre a situação de seu pai e fala de estratégia sobre como convencer um terceiro ministro, Gilmar Mendes. Os ministros citados negaram à imprensa ter tratado com Delcídio sobre o tema.  Na peça de 23 páginas da defesa prévia do senador, subscrita pelos advogados Gilson Dipp, que foi ministro do STJ (Superior Tribunal da Justiça), Luís Henrique Machado e Raul Amaral Júnior, o senador negou ter mantido as conversas com os ministros do STF, pois \"nem o senador acusado tem esse poder institucional nem os juízes daquela corte se sujeitam a esse tipo de influência, como se percebe da gravação\".  A defesa de Delcídio afirma que \"os próprios julgadores da Suprema Corte declararam não ter recebido ou agendado com o senador qualquer reunião para tratar desse ou de qualquer tema correlato\".  Os advogados também procuram, na defesa prévia, estabelecer que o comportamento atribuído ao senador não pode ser considerado quebra de decoro parlamentar porque ocorreu fora \"do desempenho da atividade própria do mandato\".  A defesa diz que a acusação do PPS e Rede refere-se em tese ao artigo 5º do Código de Ética e Decoro Parlamentar do Senado, que prevê a perda do mandato a partir da \"prática de irregularidades graves no desempenho do mandato ou de encargos decorrentes\".  Segundo a defesa, Delcídio e Bernardo Cerveró mantinham \"uma relação de confiança\" que teria sido \"construída ao longo de décadas de amizade estreita com a família Cerveró\". Assim, dizem os advogados, Delcídio \"não atuava como parlamentar na entrevista, mas como amigo da família\". Para a defesa, \"o pano de fundo do diálogo era de uma estreita relação de confiança e amizade, de confidência\".  \"Os supostos atos –cuja realidade, repita-se, foi obtida por prova ilícita e de modo ilegal– são atos da pessoa do senador, em conversa com terceiros alheios à atividade parlamentar, sobre assunto diverso dos deveres parlamentares, deixando margem a grandes dúvidas se constituíam atos do desempenho do mandato e, mais ainda, se decorrentes de seus encargos\", afirma a defesa prévia.  Os advogados de Delcídio escreveram que \"qualquer crime atribuído a um senador\" não pode ser diretamente ligado ao desempenho do mandato, e citaram como exemplo de não incidência eventuais \"lesões corporais causadas em acidente de trânsito culposo na direção de veículo no caminho do Senado\".  A defesa atacou diversas vezes o papel de Bernardo Cerveró, caracterizado como \"um agente infiltrado e provocador, sem prévia autorização judicial\". Segundo Delcídio, ele caiu \"em uma armadilha\" acertada por Bernardo com terceiros.  \"Sua intenção [de Bernardo] não era se defender de uma eventual investida criminosa, ou até indecorosa ou antiética, mas, sim, provocar o interlocutor a pronunciar declarações comprometedoras, mediante falsa representação da realidade, para, mais tarde, utilizar-se da gravação como trunfo, a fim de entabular o acordo de colaboração de seu pai, Nestor Cerveró\", dizem os advogados.  Além da defesa prévia, os advogados de Delcídio também pedirão, em documento separado, a substituição do relator do pedido de abertura do processo de cassação, Ataídes Oliveira (PSDB-TO), sob acusação de falta de isenção por ele integrar o bloco partidário da oposição, conforme a Folha antecipou nesta segunda-feira (15). Ataídes nega estar impedido de atuar no caso.  Os dois documentos serão recebidos pelo presidente do Conselho de Ética, João Alberto (PMDB-MA), que poderá submetê-los à avaliação do plenário do Conselho.  A Folha apurou que Delcídio trabalha com prioridade para tentar um habeas corpus do STF e, assim, em liberdade poder se defender pessoalmente no Conselho de Ética, buscando o apoio de cada colega senador. O pior cenário para o parlamentar, avaliam seus defensores, é enfrentar um processo de cassação ainda atrás das grades.',\n",
       " 'A presidente do Chile, Michelle Bachelet, assinou um decreto que autoriza a venda de medicamentos derivados de cannabis em farmácias.  A medida já era esperada e foi vista como parte de um esforço das autoridades chilenas para rever a legislação sobre a maconha.  O porte da droga para uso pessoal foi descriminalizado em 2007. O plantio, a venda e o transporte de maconha continuam proibidos, com penas previstas de até 15 anos de prisão.  A assinatura do decreto ocorreu no último dia 1º, mas só foi divulgada neste sábado (5). Segundo o texto, o Instituto de Saúde Pública poderá autorizar e controlar o uso de cannabis, resina de cannabis, extratos e tinturas de cannabis para a elaboração de produtos farmacêuticos de uso humano.  Os medicamentos com a substância poderão ser vendidos em farmácias ou laboratórios mediante receita médica, que ficará retida para controle.  O documento também retira a maconha da lista 1 de drogas e a incorpora na lista 2, com substâncias como a codeína. O Ministério do Interior, contudo, não explicou se essa mudança significa efetivamente a retirada definitiva da cannabis da lista de drogas pesadas.  A maconha voltou ao destaque na imprensa chilena no mês passado, quando um tribunal de família proibiu uma mãe de amamentar sua filha recém-nascida após ela admitir que fumou a substância antes do parto. A criança ficou 12 dias retida em um hospital na cidade de Talcahuano, cerca de 550 km ao sul de Santiago, antes de ser devolvida à mãe, que apelou contra o hospital na Corte Interamericana de Direitos Humanos.  O tribunal exigiu que a mãe faça testes semanais para determinar se consome maconha.  Em julho, a Câmara dos Deputados aprovou o início da discussão de um projeto que pretende legalizar o cultivo particular de maconha para consumo medicinal e recreativo, na tentativa de descriminalizar o uso da droga e conter o tráfico no país.  A medida será analisada por uma comissão parlamentar de Saúde para, então, ser votada no plenário da Câmara e no Senado. Há poucos dias, o governo enviou ao Congresso pedido para reduzir a quantidade de maconha permitida para porte e cultivo no projeto, de dez gramas para porte e dez plantas para cultivo, para dois gramas para porte e uma planta para cultivo.  O município de La Florida, no distrito de Santiago, iniciou em outubro de 2014 o plantio de maconha medicinal, como parte de um projeto-piloto aprovado pelo governo direcionado para pacientes com câncer. A primeira colheita foi em abril deste ano.  Em 2013, o Uruguai se tornou o primeiro país da América Latina a aprovar uma lei que permite a produção e venda legal de maconha, iniciativa que tem o objetivo de reduzir a insegurança associada ao narcotráfico.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee201732-3a7b-43e2-b16c-0c06fde3de83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f9000-1bf3-41a8-9b67-e3137078f66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c01146-cb81-4128-9add-9bb0bbe3baba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁três', 0.8723641037940979),\n",
       " ('▁oito', 0.8701666593551636),\n",
       " ('▁cinco', 0.8675403594970703),\n",
       " ('▁nove', 0.8654859662055969),\n",
       " ('▁seis', 0.8631857633590698),\n",
       " ('▁quatro', 0.8607264161109924),\n",
       " ('▁dez', 0.8575947284698486),\n",
       " ('▁sete', 0.8480710387229919),\n",
       " ('▁doze', 0.8359367251396179),\n",
       " ('▁vários', 0.828758180141449)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.most_similar(\"▁dois\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77186044-bddb-490c-a7c1-4b539bfa9147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.3468242e+00,  6.8137341e+00, -1.4643404e+00, -1.1822750e-01,\n",
       "       -3.8878152e+00, -3.4637120e-01,  3.0803206e+00, -6.8627906e+00,\n",
       "        3.5182815e+00,  3.8290794e+00, -8.5138006e+00, -2.4277949e+00,\n",
       "       -1.4926243e+00, -5.6872697e+00, -8.4092039e-01,  1.0705171e+00,\n",
       "        5.4289937e+00, -3.4295261e+00, -1.2320751e+00, -5.1659662e-01,\n",
       "        5.3019896e+00,  1.5884974e+00, -5.4626627e+00,  5.6278157e+00,\n",
       "        1.9585477e+00, -2.8322184e-01,  1.2161356e+00, -3.4904180e+00,\n",
       "        8.0094612e-01, -6.4040214e-02, -4.4229865e-01,  2.2009010e+00,\n",
       "        3.4180372e+00, -2.2932494e+00,  1.4665638e+01, -3.2469783e+00,\n",
       "       -1.5051919e+00,  4.7470379e-01, -2.3170009e+00,  7.3689550e-02,\n",
       "       -8.5182238e+00,  3.4171028e+00, -7.9247413e+00, -2.2553673e+00,\n",
       "        1.8577051e+00,  4.4309014e-05,  1.7016845e+00,  4.8755603e+00,\n",
       "       -1.0073901e+00, -6.7963362e+00], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.get_vector(\"▁homem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c45f5ea-38dd-41aa-a82c-f7c2f68ab21f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁rapaz', 0.8976976275444031),\n",
       " ('▁garoto', 0.8968115448951721),\n",
       " ('▁menino', 0.8940575122833252),\n",
       " ('▁assassino', 0.8306339383125305),\n",
       " ('▁pinguim', 0.8259885311126709),\n",
       " ('▁monstro', 0.8233861327171326),\n",
       " ('▁soldado', 0.8025341629981995),\n",
       " ('▁assaltante', 0.7978926301002502),\n",
       " ('▁felino', 0.7970114350318909),\n",
       " ('▁cão', 0.7948576211929321)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.most_similar(\"▁homem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b296154-f535-4c1f-b3d2-cb3e8b248a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁mãe', 0.9230387806892395),\n",
       " ('▁filha', 0.9159747958183289),\n",
       " ('▁esposa', 0.9079855680465698),\n",
       " ('▁irmã', 0.90696781873703),\n",
       " ('▁namorada', 0.901147723197937),\n",
       " ('▁amiga', 0.8933922052383423),\n",
       " ('▁menina', 0.8767526149749756),\n",
       " ('▁enteada', 0.8766278028488159),\n",
       " ('▁avó', 0.875297486782074),\n",
       " ('▁companheira', 0.8721087574958801)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.most_similar(\"▁mulher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43dad988-b3f8-41f5-a18a-ae0c62bfc1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁veículo', 0.9137380123138428),\n",
       " ('▁caminhão', 0.9056364893913269),\n",
       " ('▁motorista', 0.8465127348899841),\n",
       " ('▁helicóptero', 0.8387037515640259),\n",
       " ('▁carrinho', 0.8258286714553833),\n",
       " ('▁elevador', 0.8252330422401428),\n",
       " ('▁barraco', 0.8244249820709229),\n",
       " ('▁pneu', 0.821903645992279),\n",
       " ('▁vagão', 0.8143818378448486),\n",
       " ('▁barco', 0.814140260219574)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.most_similar(\"▁carro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb22fb8-8094-4987-a929-e6ba41bb657c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁helicóptero', 0.9103664755821228),\n",
       " ('▁navio', 0.8810691833496094),\n",
       " ('▁voo', 0.8689661026000977),\n",
       " ('▁barco', 0.8431477546691895),\n",
       " ('▁caminhão', 0.81184983253479),\n",
       " ('▁carro', 0.8110244274139404),\n",
       " ('▁drone', 0.8100888133049011),\n",
       " ('▁cargueiro', 0.8027729988098145),\n",
       " ('▁comboio', 0.7944729328155518),\n",
       " ('▁veículo', 0.7877556681632996)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.most_similar(\"▁avião\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d6dfd58-958a-425d-a1b3-deec4f1b7b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁réveillon', 0.8524163365364075),\n",
       " ('▁sambódromo', 0.8183432817459106),\n",
       " ('▁desfile', 0.8170649409294128),\n",
       " ('▁baile', 0.7560688257217407),\n",
       " ('▁círio', 0.7525671720504761),\n",
       " ('▁feriado', 0.7357999682426453),\n",
       " ('▁show', 0.7356415390968323),\n",
       " ('▁palco', 0.7276540994644165),\n",
       " ('▁samba', 0.7203671336174011),\n",
       " ('▁verão', 0.7199791073799133)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.most_similar(\"▁carnaval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1de2fa29-b440-44d7-9010-4772d896220b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁palmeiras', 0.9916632771492004),\n",
       " ('▁flamengo', 0.9533402919769287),\n",
       " ('▁grêmio', 0.946609377861023),\n",
       " ('▁audax', 0.9291609525680542),\n",
       " ('▁vasco', 0.926048219203949),\n",
       " ('▁coritiba', 0.9118771553039551),\n",
       " ('▁sport', 0.8998237252235413),\n",
       " ('▁figueirense', 0.8963984847068787),\n",
       " ('▁barça', 0.8899548053741455),\n",
       " ('▁avaí', 0.8875383138656616)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.most_similar(\"▁corinthians\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ec4875-b8c5-46ab-97e0-4fe3e3b80408",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8110244"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.similarity(\"▁carro\", \"▁avião\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f70eaa-cb2f-4fd0-b4dd-fa4bc9daa631",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48851633"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.similarity(\"▁velho\", \"▁homem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cc06b87-c769-472e-81cf-f3e5651ea21b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33917713"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.similarity(\"▁velha\", \"▁mulher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2e60055-37fa-4ee0-9ad5-f6eb14119e73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53674597"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.similarity(\"▁novela\", \"▁globo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8fc4065-a29d-4e6d-8282-a4cd03933971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁navio', 0.8869611024856567),\n",
       " ('▁avião', 0.8431476950645447),\n",
       " ('▁trem', 0.8282049298286438),\n",
       " ('▁jipe', 0.8266261219978333),\n",
       " ('▁caminhão', 0.8176475167274475),\n",
       " ('▁veleiro', 0.8168447017669678),\n",
       " ('▁carro', 0.814140260219574),\n",
       " ('▁helicóptero', 0.7995263338088989),\n",
       " ('▁píer', 0.7976621389389038),\n",
       " ('▁voo', 0.7964310646057129)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.most_similar(\"▁barco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0754415c-f7b6-47a0-8de3-e39b8f5b9a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁laptop', 0.8611619472503662),\n",
       " ('▁tablet', 0.8534693717956543),\n",
       " ('▁celular', 0.8467183709144592),\n",
       " ('▁chip', 0.8394709229469299),\n",
       " ('▁computador', 0.8318821787834167),\n",
       " ('▁smartphone', 0.8209793567657471),\n",
       " ('▁envelope', 0.8171899318695068),\n",
       " ('▁carregador', 0.7928250432014465),\n",
       " ('▁ipad', 0.7898997068405151),\n",
       " ('▁aparelho', 0.7794108390808105)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.most_similar(\"▁notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28358335-81c1-4238-a6de-aa24d94651e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁pedra', 0.8377714157104492),\n",
       " ('▁poça', 0.826803982257843),\n",
       " ('▁cratera', 0.8137418627738953),\n",
       " ('▁lona', 0.8065041303634644),\n",
       " ('▁poeira', 0.7948131561279297),\n",
       " ('▁cabana', 0.788944661617279),\n",
       " ('▁laje', 0.7798771858215332),\n",
       " ('▁montanha', 0.7764739394187927),\n",
       " ('▁escada', 0.7695632576942444),\n",
       " ('▁parede', 0.7683671116828918)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.wv.most_similar(\"▁árvore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c1a77-ff46-45fc-b44e-36247db2b8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
